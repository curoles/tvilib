// AUTOGENERATED

/// Reduction xor of all elements.
///
///
template<typename T, std::size_t VLEN = 512>
T array_xor(const T* array, std::size_t nr_elem)
{
    using AI = array_info<T,VLEN>;
    AI a(nr_elem);
    using VT = AI::VT;

    T res { 0  };

#if 0
    for (std::size_t i = 0; i < a.nr_chunks; ++i) {
        VT sv = *(VT*)&array[i*a.NR_ELEM];
        res ^= tvx::hxor(sv)[0];
    }
#else
    for (std::size_t i = 0; (i+1) < a.nr_chunks; i += 2) {
        VT sv1 = *(VT*)&array[i*a.NR_ELEM];
        VT sv2 = *(VT*)&array[(i+1)*a.NR_ELEM];
        res ^= tvx::hxor(sv1)[0] ^ tvx::hxor(sv2)[0];
    }
    if (a.nr_chunks % 2) {
        VT sv = *(VT*)&array[(a.nr_chunks-1)*a.NR_ELEM];
        res ^= tvx::hxor(sv)[0];
    }
#endif

    //FIXME TODO once intrinsic has mask, rewrite with mask
    for (std::size_t i = a.nr_chunks*a.NR_ELEM; i < a.nr_elem; ++i) {
        res ^= array[i];
    }
    /*if (a.nr_tail_elem) {
        tvx::pmask_t mask = tvx::_pmask(a.nr_tail_elem * sizeof(T));
        VT sv = tvx::vld(mask, (VT*)&array[a.nr_chunks*a.NR_ELEM]);
        res &= tvx::hand(sv)[0]; //FIXME use mask
    }*/

    return res;
}

/// Reduction xor of a contiguous sequence of objects.
///
/// span can be C array or STL container.
///
template<typename T, std::size_t VLEN = 512>
T array_xor(std::span<const T> array)
{
    return vil::array_xor<T,VLEN>(array.data(), array.size());
}
